{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('creditcard.csv', names = (0,31), header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                                                                                                                                                                                                                                                                   0   \\\n",
       "0    1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724     2.69   \n",
       "1   -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   378.66   \n",
       "    -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   123.50   \n",
       "2   -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153    69.99   \n",
       "    -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201  0.260314 -0.568671 -0.371407  1.341262  0.359894 -0.358091 -0.137134  0.517617  0.401726 -0.058133  0.068653 -0.033194  0.084968 -0.208254 -0.559825 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080     3.67   \n",
       "4    1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159  0.081213  0.464960 -0.099254 -1.416907 -0.153826 -0.751063  0.167372  0.050144 -0.443587  0.002821 -0.611987 -0.045575 -0.219633 -0.167716 -0.270710 -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168     4.99   \n",
       "7   -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631 -3.807864  0.615375  1.249376 -0.619468  0.291474  1.757964 -1.323865  0.686133 -0.076127 -1.222127 -0.358222  0.324505 -0.156742  1.943465 -1.015455  0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339    40.80   \n",
       "    -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145  0.851084 -0.392048 -0.410430 -0.705117 -0.110452 -0.286254  0.074355 -0.328783 -0.210077 -0.499768  0.118765  0.570328  0.052736 -0.073425 -0.268092 -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404    93.20   \n",
       "9   -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583  0.069539 -0.736727 -0.366846  1.017614  0.836390  1.006844 -0.443523  0.150219  0.739453 -0.540980  0.476677  0.451773  0.203711 -0.246914 -0.633753 -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076     3.68   \n",
       "10   1.449044 -1.176339  0.913860 -1.375667 -1.971383 -0.629152 -1.423236  0.048456 -1.720408  1.626659  1.199644 -0.671440 -0.513947 -0.095045  0.230930  0.031967  0.253415  0.854344 -0.221365 -0.387226 -0.009302  0.313894  0.027740  0.500512  0.251367 -0.129478  0.042850  0.016253     7.80   \n",
       "     0.384978  0.616109 -0.874300 -0.094019  2.924584  3.317027  0.470455  0.538247 -0.558895  0.309755 -0.259116 -0.326143 -0.090047  0.362832  0.928904 -0.129487 -0.809979  0.359985  0.707664  0.125992  0.049924  0.238422  0.009130  0.996710 -0.767315 -0.492208  0.042472 -0.054337     9.99   \n",
       "     1.249999 -1.221637  0.383930 -1.234899 -1.485419 -0.753230 -0.689405 -0.227487 -2.094011  1.323729  0.227666 -0.242682  1.205417 -0.317631  0.725675 -0.815612  0.873936 -0.847789 -0.683193 -0.102756 -0.231809 -0.483285  0.084668  0.392831  0.161135 -0.354990  0.026416  0.042422   121.50   \n",
       "11   1.069374  0.287722  0.828613  2.712520 -0.178398  0.337544 -0.096717  0.115982 -0.221083  0.460230 -0.773657  0.323387 -0.011076 -0.178485 -0.655564 -0.199925  0.124005 -0.980496 -0.982916 -0.153197 -0.036876  0.074412 -0.071407  0.104744  0.548265  0.104094  0.021491  0.021293    27.50   \n",
       "12  -2.791855 -0.327771  1.641750  1.767473 -0.136588  0.807596 -0.422911 -1.907107  0.755713  1.151087  0.844555  0.792944  0.370448 -0.734975  0.406796 -0.303058 -0.155869  0.778265  2.221868 -1.582122  1.151663  0.222182  1.020586  0.028317 -0.232746 -0.235557 -0.164778 -0.030154    58.80   \n",
       "    -0.752417  0.345485  2.057323 -1.468643 -1.158394 -0.077850 -0.608581  0.003603 -0.436167  0.747731 -0.793981 -0.770407  1.047627 -1.066604  1.106953  1.660114 -0.279265 -0.419994  0.432535  0.263451  0.499625  1.353650 -0.256573 -0.065084 -0.039124 -0.087086 -0.180997  0.129394    15.99   \n",
       "     1.103215 -0.040296  1.267332  1.289091 -0.735997  0.288069 -0.586057  0.189380  0.782333 -0.267975 -0.450311  0.936708  0.708380 -0.468647  0.354574 -0.246635 -0.009212 -0.595912 -0.575682 -0.113910 -0.024612  0.196002  0.013802  0.103758  0.364298 -0.382261  0.092809  0.037051    12.99   \n",
       "13  -0.436905  0.918966  0.924591 -0.727219  0.915679 -0.127867  0.707642  0.087962 -0.665271 -0.737980  0.324098  0.277192  0.252624 -0.291896 -0.184520  1.143174 -0.928709  0.680470  0.025436 -0.047021 -0.194796 -0.672638 -0.156858 -0.888386 -0.342413 -0.049027  0.079692  0.131024     0.89   \n",
       "14  -5.401258 -5.450148  1.186305  1.736239  3.049106 -1.763406 -1.559738  0.160842  1.233090  0.345173  0.917230  0.970117 -0.266568 -0.479130 -0.526609  0.472004 -0.725481  0.075081 -0.406867 -2.196848 -0.503600  0.984460  2.458589  0.042119 -0.481631 -0.621272  0.392053  0.949594    46.80   \n",
       "15   1.492936 -1.029346  0.454795 -1.438026 -1.555434 -0.720961 -1.080664 -0.053127 -1.978682  1.638076  1.077542 -0.632047 -0.416957  0.052011 -0.042979 -0.166432  0.304241  0.554432  0.054230 -0.387910 -0.177650 -0.175074  0.040002  0.295814  0.332931 -0.220385  0.022298  0.007602     5.00   \n",
       "16   0.694885 -1.361819  1.029221  0.834159 -1.191209  1.309109 -0.878586  0.445290 -0.446196  0.568521  1.019151  1.298329  0.420480 -0.372651 -0.807980 -2.044557  0.515663  0.625847 -1.300408 -0.138334 -0.295583 -0.571955 -0.050881 -0.304215  0.072001 -0.422234  0.086553  0.063499   231.71   \n",
       "17   0.962496  0.328461 -0.171479  2.109204  1.129566  1.696038  0.107712  0.521502 -1.191311  0.724396  1.690330  0.406774 -0.936421  0.983739  0.710911 -0.602232  0.402484 -1.737162 -2.027612 -0.269321  0.143997  0.402492 -0.048508 -1.371866  0.390814  0.199964  0.016371 -0.014605    34.09   \n",
       "18   1.166616  0.502120 -0.067300  2.261569  0.428804  0.089474  0.241147  0.138082 -0.989162  0.922175  0.744786 -0.531377 -2.105346  1.126870  0.003075  0.424425 -0.454475 -0.098871 -0.816597 -0.307169  0.018702 -0.061972 -0.103855 -0.370415  0.603200  0.108556 -0.040521 -0.011418     2.28   \n",
       "     0.247491  0.277666  1.185471 -0.092603 -1.314394 -0.150116 -0.946365 -1.617935  1.544071 -0.829881 -0.583200  0.524933 -0.453375  0.081393  1.555204 -1.396895  0.783131  0.436621  2.177807 -0.230983  1.650180  0.200454 -0.185353  0.423073  0.820591 -0.227632  0.336634  0.250475    22.75   \n",
       "22  -1.946525 -0.044901 -0.405570 -1.013057  2.941968  2.955053 -0.063063  0.855546  0.049967  0.573743 -0.081257 -0.215745  0.044161  0.033898  1.190718  0.578843 -0.975667  0.044063  0.488603 -0.216715 -0.579526 -0.799229  0.870300  0.983421  0.321201  0.149650  0.707519  0.014600     0.89   \n",
       "    -2.074295 -0.121482  1.322021  0.410008  0.295198 -0.959537  0.543985 -0.104627  0.475664  0.149451 -0.856566 -0.180523 -0.655233 -0.279797 -0.211668 -0.333321  0.010751 -0.488473  0.505751 -0.386694 -0.403639 -0.227404  0.742435  0.398535  0.249212  0.274404  0.359969  0.243232    26.43   \n",
       "23   1.173285  0.353498  0.283905  1.133563 -0.172577 -0.916054  0.369025 -0.327260 -0.246651 -0.046139 -0.143419  0.979350  1.492285  0.101418  0.761478 -0.014584 -0.511640 -0.325056 -0.390934  0.027878  0.067003  0.227812 -0.150487  0.435045  0.724825 -0.337082  0.016368  0.030041    41.88   \n",
       "     1.322707 -0.174041  0.434555  0.576038 -0.836758 -0.831083 -0.264905 -0.220982 -1.071425  0.868559 -0.641506 -0.111316  0.361485  0.171945  0.782167 -1.355871 -0.216935  1.271765 -1.240622 -0.522951 -0.284376 -0.323357 -0.037710  0.347151  0.559639 -0.280158  0.042335  0.028822    16.00   \n",
       "    -0.414289  0.905437  1.727453  1.473471  0.007443 -0.200331  0.740228 -0.029247 -0.593392 -0.346188 -0.012142  0.786796  0.635954 -0.086324  0.076804 -1.405919  0.775592 -0.942889  0.543969  0.097308  0.077237  0.457331 -0.038500  0.642522 -0.183891 -0.277464  0.182687  0.152665    33.00   \n",
       "     1.059387 -0.175319  1.266130  1.186110 -0.786002  0.578435 -0.767084  0.401046  0.699500 -0.064738  1.048292  1.005618 -0.542002 -0.039915 -0.218683  0.004476 -0.193554  0.042388 -0.277834 -0.178023  0.013676  0.213734  0.014462  0.002951  0.294638 -0.395070  0.081461  0.024220    12.99   \n",
       "24   1.237429  0.061043  0.380526  0.761564 -0.359771 -0.494084  0.006494 -0.133862  0.438810 -0.207358 -0.929182  0.527106  0.348676 -0.152535 -0.218386 -0.191552 -0.116581 -0.633791  0.348416 -0.066351 -0.245682 -0.530900 -0.044265  0.079168  0.509136  0.288858 -0.022705  0.011836    17.28   \n",
       "...                                                                                                                                                                                                                                                                                              ...   \n",
       "481 -2.752124 -3.232168  0.873036  0.108217 -2.451279  0.486159  3.152907 -1.013087  1.128326 -0.939778 -0.490741  0.096216 -0.043465 -1.140294 -0.728960  0.199869 -0.399117 -0.603039 -0.274479 -0.102130 -0.299791 -0.003767  0.545794  0.853981 -0.943651  0.607826  0.335131 -0.561721  1015.61   \n",
       "     1.215468 -0.071342  1.048697  0.724719 -0.694532  0.041879 -0.528919  0.019939  0.735112 -0.285616 -0.951484  1.078950  1.672256 -0.743839  0.071255  0.247985 -0.457251 -0.248081  0.150889  0.020419 -0.107924 -0.044997 -0.055459 -0.062254  0.377130  0.344304  0.025717  0.025628    11.50   \n",
       "483  1.039674 -0.329296  1.073431  1.547480 -0.595510  1.090970 -0.703763  0.511730  1.176992 -0.220267  0.113150  0.986915 -1.375333 -0.347843 -2.260052 -0.802282  0.466460 -0.616352  0.591883 -0.236056 -0.391770 -0.761528  0.035646 -0.329862  0.399210 -0.504260  0.078070  0.014683    20.74   \n",
       "    -0.792724  0.946688  2.245149  1.167398 -0.625405 -0.449979  0.157625  0.015379  0.269781 -0.021304 -0.375567  0.478018  0.287826 -0.473544  0.127464 -0.680594  0.223911  0.014717  0.365767  0.114467  0.223032  0.926665 -0.170298  0.972432 -0.115298 -0.229900  0.156610  0.252310     4.42   \n",
       "484 -1.123243 -0.601733  1.866232  0.373544 -0.165149  1.602099 -0.220908  0.618353 -1.172353  0.315664  0.318390  0.479869  0.786431 -0.289770  1.475877 -3.205597  1.478728 -0.376979 -2.028647  0.084833  0.090582  0.804918  0.432421 -0.658489 -0.385666 -0.068157  0.459111  0.222762   158.00   \n",
       "     1.093370  0.108995  0.100227  0.913194  0.084986  0.030469  0.087877  0.064277 -0.256511  0.120404  1.225781  0.830764 -0.088574  0.614135  0.505950  0.056696 -0.513619  0.016764 -0.330150 -0.064301  0.112292  0.265242 -0.162023 -0.294748  0.610165 -0.300513  0.017064  0.011275    49.90   \n",
       "    -0.928088  0.398194  1.741131  0.182673  0.966387 -0.901004  0.879016 -0.156590 -0.142117 -0.574775 -0.478181 -0.426852 -1.493426  0.187921 -0.282355 -0.648150 -0.019449 -0.782172 -1.194998 -0.118283  0.066353  0.281378 -0.257966  0.385384  0.391117 -0.453853 -0.104448 -0.125765     1.00   \n",
       "     1.060623  0.077500 -0.049338  0.546163  0.004831 -0.732323  0.547012 -0.275000 -0.374239 -0.176167  0.210358  0.977909  1.163019  0.304780  0.898922 -0.307363 -0.032708 -1.451618 -0.287768  0.127655 -0.286497 -0.982239  0.080482  0.097460  0.217021  0.154499 -0.057598  0.024727    99.90   \n",
       "487 -0.517981  0.723072  1.401196  0.772313  0.671651  0.847780  0.352294  0.224071 -2.314242  0.941669 -2.194867 -1.906235 -0.940699  0.306290  0.646916 -0.869121 -0.422321  1.417456 -1.335051 -0.517548 -0.374926 -0.737659 -0.391401 -1.420024  0.435491  0.192061  0.051327  0.052402     7.48   \n",
       "     1.164359 -0.283474 -0.247022  0.192064 -0.327522 -0.822581  0.241389 -0.228467  0.395383 -0.201542 -1.085632 -0.309149 -0.813103  0.316679  0.240161 -0.100465 -0.075822 -0.474612  0.463448  0.045042 -0.077166 -0.311768 -0.214784 -0.041134  0.552795  1.114887 -0.116519  0.001287    91.76   \n",
       "     0.174857  2.035550 -1.866661  1.420853  0.829644 -1.683505  1.014476 -0.303887  0.196620  0.129235  2.643747 -0.317019 -1.015285 -3.601836  0.517658  0.862953  2.556382  1.950361 -0.282787  0.508856 -0.233184 -0.111163  0.093547  0.159321 -0.414132 -0.443439  0.409371 -0.042075     8.99   \n",
       "488  0.986168 -0.550923  1.140934  0.757901 -1.038890  0.476782 -0.837096  0.433228  0.896975 -0.095754  0.919861  0.386846 -1.608431  0.140629 -0.070363  0.148192 -0.057059  0.014333 -0.115602 -0.106760  0.003400 -0.001184  0.017652  0.026666  0.078602  0.328056  0.008762  0.020131    64.99   \n",
       "489 -0.932349  0.789123  0.534111  1.140724 -0.448967 -0.288526  1.072543  0.118515 -0.152883 -0.033482 -0.482043 -0.315406 -1.218820  0.505762  0.456441 -1.041383  0.600218 -0.563253  0.193950 -0.099108  0.092532  0.662627  0.319256  0.397955 -0.454037 -0.338746  0.134917 -0.094342   139.75   \n",
       "490 -0.215043  0.652190  1.107882  1.265935  2.366300  4.393732 -0.412469  0.960644 -0.914003  0.745979 -0.388455 -0.683481  0.174722 -0.197217  1.423126  0.906270 -1.086627  0.907024  0.226867  0.266120  0.103830  0.304199 -0.127455  0.994761 -0.501053  0.039003  0.024135 -0.056241     3.44   \n",
       "     0.716662 -1.097812  0.629443  1.939510 -0.494739  1.819965 -0.624840  0.482657  0.783612  0.207105 -1.392283  0.330364 -0.574920 -0.737417 -2.643564  0.604758 -0.450969  0.219757  0.967780  0.388818 -0.263286 -0.978693 -0.359244 -1.332021  0.296656  0.937425 -0.068357  0.032839   254.41   \n",
       "491 -0.946412  0.609500  1.201710  0.113074 -0.210132 -0.954009  0.130325  0.293270 -0.579076 -0.756168 -0.627040  0.495057  0.646681  0.227179  0.319772 -0.368821  0.386365 -0.537638  1.125068  0.144104 -0.036421 -0.266529 -0.038869  0.481943 -0.277006  0.949985 -0.079616 -0.016606    18.45   \n",
       "     1.196288  0.217099  0.402596  0.921664 -0.252045 -0.392240 -0.047177  0.041339 -0.069318  0.169913  0.947708  0.417596 -0.798841  0.679509  0.453897  0.562062 -0.805390  0.367828  0.128334 -0.157752 -0.205751 -0.688869  0.054388 -0.056469  0.352684 -0.625998  0.013681  0.015683     9.99   \n",
       "     1.130024 -0.060944  0.662074  1.033990 -0.441560  0.310187 -0.559948  0.349136  0.332922  0.220886  0.826773 -0.257591 -1.961189  0.728867  1.004105  0.527214 -0.583410  0.631457 -0.573334 -0.258810  0.195852  0.511310 -0.100457 -0.344298  0.420079 -0.213097  0.040658  0.010599    10.00   \n",
       "492 -0.789890 -1.379371  0.171334 -1.636756 -2.807266  0.726236  2.737602 -0.933999 -2.413730  0.796625 -0.878071 -1.045044  1.249929 -0.960840 -0.507105 -0.831644  0.490803 -0.077389  0.298180 -0.129411 -0.256231  0.167077  0.540876  0.067496  0.479636 -0.098230  0.053527 -0.408050   632.40   \n",
       "493  1.219725 -0.481149 -0.324351 -1.552562 -0.258357 -0.519342 -0.035458 -0.117824  1.267125 -1.068542 -0.638446  0.352485  0.060428  0.332823  1.999236 -0.505571 -0.160606 -0.332378  0.646091  0.035718 -0.040428 -0.106938 -0.171758 -0.721597  0.525568  0.084212 -0.001621  0.011866    67.94   \n",
       "494  0.058501  1.058661  1.076857  1.589043 -0.400077 -0.677461  0.778466 -0.303557 -0.630844  0.230737  0.015912  0.549293  1.123662  0.146946  1.428015 -0.697686  0.108457 -0.015595  1.176153  0.104401  0.192879  0.702194  0.217738  0.736017 -1.276798 -0.461654  0.205261  0.250664    54.99   \n",
       "495 -0.239505 -3.940241 -0.147576 -0.671347 -2.239256  0.908178 -0.377398  0.157943 -1.595928  0.987881  1.225026 -0.139996 -0.453948 -0.111467 -0.490358 -0.973998  1.248488 -0.588332 -0.509182  1.217690  0.076296 -1.132178 -0.486820 -0.302911 -0.304121 -0.469811 -0.077517  0.151745   834.84   \n",
       "496 -0.606284 -0.090112  2.012027  0.352728 -0.210154  0.513719 -0.496402  0.379687 -1.663990  0.767278  0.743764 -0.250209 -0.405640  0.312433  1.090898 -1.509169 -0.009483  2.107139 -0.172341 -0.250196 -0.065436  0.148715 -0.060198 -0.356965 -0.213996 -0.143624  0.167360  0.122290    29.00   \n",
       "    -2.009893  0.522693  2.043510  0.223521 -0.597859  0.078163 -0.450347  0.772471  0.532385 -0.408490 -0.862066 -0.035498 -0.648890 -0.191864  0.423878 -0.106994  0.332679 -0.405176 -0.141551 -0.150668 -0.126642  0.080104  0.272598  0.088534  0.317082  0.419253  0.281494  0.060872    16.50   \n",
       "    -0.652955  0.493216  1.246770 -0.707569  0.165764 -1.146224  0.932049 -0.094962 -0.861258 -0.503549  1.448318  0.352719 -0.832586  0.721808 -0.002442  0.060623 -0.270581 -0.285535  0.757072  0.117608 -0.209224 -0.894706  0.167075  0.528922 -0.419801  0.629628 -0.040930  0.097624    50.99   \n",
       "497 -0.417836  0.981103  1.135820 -0.172593  0.396675 -0.002842  0.519780  0.248763 -0.593497 -0.382215  1.364965  0.429958 -0.220197 -0.165067  0.501964  0.396923 -0.066857  0.053123 -0.035596  0.098874 -0.205580 -0.547474 -0.020486 -0.368187 -0.227053  0.105281  0.253800  0.081276     8.83   \n",
       "498 -0.753769  1.098287  1.835453  0.614571 -0.317859 -0.101055  0.039454 -0.656824 -0.048761 -0.216191 -0.311959 -0.414791 -1.023195  0.270999  1.248325 -0.046737 -0.097763 -0.015157 -0.965601 -0.369512  1.101142  0.686059  0.005954  0.361744 -0.457852 -0.352576  0.094602  0.170480     5.00   \n",
       "499  1.255439  0.307729  0.292700  0.699873 -0.428876 -1.088456  0.043840 -0.167739  0.128854 -0.264062 -0.174508 -0.103537 -0.410713 -0.160753  1.163808  0.508362  0.031183 -0.221105 -0.194241 -0.121156 -0.294795 -0.882126  0.136846  0.327949  0.194459  0.096516 -0.027271  0.029491     1.98   \n",
       "     1.355790 -1.122921  1.191187 -0.576548 -1.708750  0.274721 -1.640785  0.295838  0.618649  0.475943 -1.772232 -1.558247 -1.276128 -0.750821  0.394213  1.563193  0.139541 -0.368044  0.765654  0.020548  0.302035  0.920013 -0.210219 -0.435499  0.450803  0.068454  0.062214  0.022191    22.83   \n",
       "    -0.860626 -0.109137  2.112474 -1.400567  0.180269  1.329656  0.431001  0.317761  0.593762 -0.987304 -0.309227  0.067846 -0.841815 -0.609413 -1.189738  0.329121 -0.627299  0.012316  0.257628  0.157201 -0.197635 -0.436535 -0.194840 -1.236873  0.218668  0.902383 -0.210195 -0.190458    92.82   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                             31  \n",
       "0    1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   0  \n",
       "1   -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   0  \n",
       "    -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   0  \n",
       "2   -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   0  \n",
       "    -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201  0.260314 -0.568671 -0.371407  1.341262  0.359894 -0.358091 -0.137134  0.517617  0.401726 -0.058133  0.068653 -0.033194  0.084968 -0.208254 -0.559825 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080   0  \n",
       "4    1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159  0.081213  0.464960 -0.099254 -1.416907 -0.153826 -0.751063  0.167372  0.050144 -0.443587  0.002821 -0.611987 -0.045575 -0.219633 -0.167716 -0.270710 -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168   0  \n",
       "7   -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631 -3.807864  0.615375  1.249376 -0.619468  0.291474  1.757964 -1.323865  0.686133 -0.076127 -1.222127 -0.358222  0.324505 -0.156742  1.943465 -1.015455  0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   0  \n",
       "    -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145  0.851084 -0.392048 -0.410430 -0.705117 -0.110452 -0.286254  0.074355 -0.328783 -0.210077 -0.499768  0.118765  0.570328  0.052736 -0.073425 -0.268092 -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   0  \n",
       "9   -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583  0.069539 -0.736727 -0.366846  1.017614  0.836390  1.006844 -0.443523  0.150219  0.739453 -0.540980  0.476677  0.451773  0.203711 -0.246914 -0.633753 -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076   0  \n",
       "10   1.449044 -1.176339  0.913860 -1.375667 -1.971383 -0.629152 -1.423236  0.048456 -1.720408  1.626659  1.199644 -0.671440 -0.513947 -0.095045  0.230930  0.031967  0.253415  0.854344 -0.221365 -0.387226 -0.009302  0.313894  0.027740  0.500512  0.251367 -0.129478  0.042850  0.016253   0  \n",
       "     0.384978  0.616109 -0.874300 -0.094019  2.924584  3.317027  0.470455  0.538247 -0.558895  0.309755 -0.259116 -0.326143 -0.090047  0.362832  0.928904 -0.129487 -0.809979  0.359985  0.707664  0.125992  0.049924  0.238422  0.009130  0.996710 -0.767315 -0.492208  0.042472 -0.054337   0  \n",
       "     1.249999 -1.221637  0.383930 -1.234899 -1.485419 -0.753230 -0.689405 -0.227487 -2.094011  1.323729  0.227666 -0.242682  1.205417 -0.317631  0.725675 -0.815612  0.873936 -0.847789 -0.683193 -0.102756 -0.231809 -0.483285  0.084668  0.392831  0.161135 -0.354990  0.026416  0.042422   0  \n",
       "11   1.069374  0.287722  0.828613  2.712520 -0.178398  0.337544 -0.096717  0.115982 -0.221083  0.460230 -0.773657  0.323387 -0.011076 -0.178485 -0.655564 -0.199925  0.124005 -0.980496 -0.982916 -0.153197 -0.036876  0.074412 -0.071407  0.104744  0.548265  0.104094  0.021491  0.021293   0  \n",
       "12  -2.791855 -0.327771  1.641750  1.767473 -0.136588  0.807596 -0.422911 -1.907107  0.755713  1.151087  0.844555  0.792944  0.370448 -0.734975  0.406796 -0.303058 -0.155869  0.778265  2.221868 -1.582122  1.151663  0.222182  1.020586  0.028317 -0.232746 -0.235557 -0.164778 -0.030154   0  \n",
       "    -0.752417  0.345485  2.057323 -1.468643 -1.158394 -0.077850 -0.608581  0.003603 -0.436167  0.747731 -0.793981 -0.770407  1.047627 -1.066604  1.106953  1.660114 -0.279265 -0.419994  0.432535  0.263451  0.499625  1.353650 -0.256573 -0.065084 -0.039124 -0.087086 -0.180997  0.129394   0  \n",
       "     1.103215 -0.040296  1.267332  1.289091 -0.735997  0.288069 -0.586057  0.189380  0.782333 -0.267975 -0.450311  0.936708  0.708380 -0.468647  0.354574 -0.246635 -0.009212 -0.595912 -0.575682 -0.113910 -0.024612  0.196002  0.013802  0.103758  0.364298 -0.382261  0.092809  0.037051   0  \n",
       "13  -0.436905  0.918966  0.924591 -0.727219  0.915679 -0.127867  0.707642  0.087962 -0.665271 -0.737980  0.324098  0.277192  0.252624 -0.291896 -0.184520  1.143174 -0.928709  0.680470  0.025436 -0.047021 -0.194796 -0.672638 -0.156858 -0.888386 -0.342413 -0.049027  0.079692  0.131024   0  \n",
       "14  -5.401258 -5.450148  1.186305  1.736239  3.049106 -1.763406 -1.559738  0.160842  1.233090  0.345173  0.917230  0.970117 -0.266568 -0.479130 -0.526609  0.472004 -0.725481  0.075081 -0.406867 -2.196848 -0.503600  0.984460  2.458589  0.042119 -0.481631 -0.621272  0.392053  0.949594   0  \n",
       "15   1.492936 -1.029346  0.454795 -1.438026 -1.555434 -0.720961 -1.080664 -0.053127 -1.978682  1.638076  1.077542 -0.632047 -0.416957  0.052011 -0.042979 -0.166432  0.304241  0.554432  0.054230 -0.387910 -0.177650 -0.175074  0.040002  0.295814  0.332931 -0.220385  0.022298  0.007602   0  \n",
       "16   0.694885 -1.361819  1.029221  0.834159 -1.191209  1.309109 -0.878586  0.445290 -0.446196  0.568521  1.019151  1.298329  0.420480 -0.372651 -0.807980 -2.044557  0.515663  0.625847 -1.300408 -0.138334 -0.295583 -0.571955 -0.050881 -0.304215  0.072001 -0.422234  0.086553  0.063499   0  \n",
       "17   0.962496  0.328461 -0.171479  2.109204  1.129566  1.696038  0.107712  0.521502 -1.191311  0.724396  1.690330  0.406774 -0.936421  0.983739  0.710911 -0.602232  0.402484 -1.737162 -2.027612 -0.269321  0.143997  0.402492 -0.048508 -1.371866  0.390814  0.199964  0.016371 -0.014605   0  \n",
       "18   1.166616  0.502120 -0.067300  2.261569  0.428804  0.089474  0.241147  0.138082 -0.989162  0.922175  0.744786 -0.531377 -2.105346  1.126870  0.003075  0.424425 -0.454475 -0.098871 -0.816597 -0.307169  0.018702 -0.061972 -0.103855 -0.370415  0.603200  0.108556 -0.040521 -0.011418   0  \n",
       "     0.247491  0.277666  1.185471 -0.092603 -1.314394 -0.150116 -0.946365 -1.617935  1.544071 -0.829881 -0.583200  0.524933 -0.453375  0.081393  1.555204 -1.396895  0.783131  0.436621  2.177807 -0.230983  1.650180  0.200454 -0.185353  0.423073  0.820591 -0.227632  0.336634  0.250475   0  \n",
       "22  -1.946525 -0.044901 -0.405570 -1.013057  2.941968  2.955053 -0.063063  0.855546  0.049967  0.573743 -0.081257 -0.215745  0.044161  0.033898  1.190718  0.578843 -0.975667  0.044063  0.488603 -0.216715 -0.579526 -0.799229  0.870300  0.983421  0.321201  0.149650  0.707519  0.014600   0  \n",
       "    -2.074295 -0.121482  1.322021  0.410008  0.295198 -0.959537  0.543985 -0.104627  0.475664  0.149451 -0.856566 -0.180523 -0.655233 -0.279797 -0.211668 -0.333321  0.010751 -0.488473  0.505751 -0.386694 -0.403639 -0.227404  0.742435  0.398535  0.249212  0.274404  0.359969  0.243232   0  \n",
       "23   1.173285  0.353498  0.283905  1.133563 -0.172577 -0.916054  0.369025 -0.327260 -0.246651 -0.046139 -0.143419  0.979350  1.492285  0.101418  0.761478 -0.014584 -0.511640 -0.325056 -0.390934  0.027878  0.067003  0.227812 -0.150487  0.435045  0.724825 -0.337082  0.016368  0.030041   0  \n",
       "     1.322707 -0.174041  0.434555  0.576038 -0.836758 -0.831083 -0.264905 -0.220982 -1.071425  0.868559 -0.641506 -0.111316  0.361485  0.171945  0.782167 -1.355871 -0.216935  1.271765 -1.240622 -0.522951 -0.284376 -0.323357 -0.037710  0.347151  0.559639 -0.280158  0.042335  0.028822   0  \n",
       "    -0.414289  0.905437  1.727453  1.473471  0.007443 -0.200331  0.740228 -0.029247 -0.593392 -0.346188 -0.012142  0.786796  0.635954 -0.086324  0.076804 -1.405919  0.775592 -0.942889  0.543969  0.097308  0.077237  0.457331 -0.038500  0.642522 -0.183891 -0.277464  0.182687  0.152665   0  \n",
       "     1.059387 -0.175319  1.266130  1.186110 -0.786002  0.578435 -0.767084  0.401046  0.699500 -0.064738  1.048292  1.005618 -0.542002 -0.039915 -0.218683  0.004476 -0.193554  0.042388 -0.277834 -0.178023  0.013676  0.213734  0.014462  0.002951  0.294638 -0.395070  0.081461  0.024220   0  \n",
       "24   1.237429  0.061043  0.380526  0.761564 -0.359771 -0.494084  0.006494 -0.133862  0.438810 -0.207358 -0.929182  0.527106  0.348676 -0.152535 -0.218386 -0.191552 -0.116581 -0.633791  0.348416 -0.066351 -0.245682 -0.530900 -0.044265  0.079168  0.509136  0.288858 -0.022705  0.011836   0  \n",
       "...                                                                                                                                                                                                                                                                                          ..  \n",
       "481 -2.752124 -3.232168  0.873036  0.108217 -2.451279  0.486159  3.152907 -1.013087  1.128326 -0.939778 -0.490741  0.096216 -0.043465 -1.140294 -0.728960  0.199869 -0.399117 -0.603039 -0.274479 -0.102130 -0.299791 -0.003767  0.545794  0.853981 -0.943651  0.607826  0.335131 -0.561721   0  \n",
       "     1.215468 -0.071342  1.048697  0.724719 -0.694532  0.041879 -0.528919  0.019939  0.735112 -0.285616 -0.951484  1.078950  1.672256 -0.743839  0.071255  0.247985 -0.457251 -0.248081  0.150889  0.020419 -0.107924 -0.044997 -0.055459 -0.062254  0.377130  0.344304  0.025717  0.025628   0  \n",
       "483  1.039674 -0.329296  1.073431  1.547480 -0.595510  1.090970 -0.703763  0.511730  1.176992 -0.220267  0.113150  0.986915 -1.375333 -0.347843 -2.260052 -0.802282  0.466460 -0.616352  0.591883 -0.236056 -0.391770 -0.761528  0.035646 -0.329862  0.399210 -0.504260  0.078070  0.014683   0  \n",
       "    -0.792724  0.946688  2.245149  1.167398 -0.625405 -0.449979  0.157625  0.015379  0.269781 -0.021304 -0.375567  0.478018  0.287826 -0.473544  0.127464 -0.680594  0.223911  0.014717  0.365767  0.114467  0.223032  0.926665 -0.170298  0.972432 -0.115298 -0.229900  0.156610  0.252310   0  \n",
       "484 -1.123243 -0.601733  1.866232  0.373544 -0.165149  1.602099 -0.220908  0.618353 -1.172353  0.315664  0.318390  0.479869  0.786431 -0.289770  1.475877 -3.205597  1.478728 -0.376979 -2.028647  0.084833  0.090582  0.804918  0.432421 -0.658489 -0.385666 -0.068157  0.459111  0.222762   0  \n",
       "     1.093370  0.108995  0.100227  0.913194  0.084986  0.030469  0.087877  0.064277 -0.256511  0.120404  1.225781  0.830764 -0.088574  0.614135  0.505950  0.056696 -0.513619  0.016764 -0.330150 -0.064301  0.112292  0.265242 -0.162023 -0.294748  0.610165 -0.300513  0.017064  0.011275   0  \n",
       "    -0.928088  0.398194  1.741131  0.182673  0.966387 -0.901004  0.879016 -0.156590 -0.142117 -0.574775 -0.478181 -0.426852 -1.493426  0.187921 -0.282355 -0.648150 -0.019449 -0.782172 -1.194998 -0.118283  0.066353  0.281378 -0.257966  0.385384  0.391117 -0.453853 -0.104448 -0.125765   0  \n",
       "     1.060623  0.077500 -0.049338  0.546163  0.004831 -0.732323  0.547012 -0.275000 -0.374239 -0.176167  0.210358  0.977909  1.163019  0.304780  0.898922 -0.307363 -0.032708 -1.451618 -0.287768  0.127655 -0.286497 -0.982239  0.080482  0.097460  0.217021  0.154499 -0.057598  0.024727   0  \n",
       "487 -0.517981  0.723072  1.401196  0.772313  0.671651  0.847780  0.352294  0.224071 -2.314242  0.941669 -2.194867 -1.906235 -0.940699  0.306290  0.646916 -0.869121 -0.422321  1.417456 -1.335051 -0.517548 -0.374926 -0.737659 -0.391401 -1.420024  0.435491  0.192061  0.051327  0.052402   0  \n",
       "     1.164359 -0.283474 -0.247022  0.192064 -0.327522 -0.822581  0.241389 -0.228467  0.395383 -0.201542 -1.085632 -0.309149 -0.813103  0.316679  0.240161 -0.100465 -0.075822 -0.474612  0.463448  0.045042 -0.077166 -0.311768 -0.214784 -0.041134  0.552795  1.114887 -0.116519  0.001287   0  \n",
       "     0.174857  2.035550 -1.866661  1.420853  0.829644 -1.683505  1.014476 -0.303887  0.196620  0.129235  2.643747 -0.317019 -1.015285 -3.601836  0.517658  0.862953  2.556382  1.950361 -0.282787  0.508856 -0.233184 -0.111163  0.093547  0.159321 -0.414132 -0.443439  0.409371 -0.042075   0  \n",
       "488  0.986168 -0.550923  1.140934  0.757901 -1.038890  0.476782 -0.837096  0.433228  0.896975 -0.095754  0.919861  0.386846 -1.608431  0.140629 -0.070363  0.148192 -0.057059  0.014333 -0.115602 -0.106760  0.003400 -0.001184  0.017652  0.026666  0.078602  0.328056  0.008762  0.020131   0  \n",
       "489 -0.932349  0.789123  0.534111  1.140724 -0.448967 -0.288526  1.072543  0.118515 -0.152883 -0.033482 -0.482043 -0.315406 -1.218820  0.505762  0.456441 -1.041383  0.600218 -0.563253  0.193950 -0.099108  0.092532  0.662627  0.319256  0.397955 -0.454037 -0.338746  0.134917 -0.094342   0  \n",
       "490 -0.215043  0.652190  1.107882  1.265935  2.366300  4.393732 -0.412469  0.960644 -0.914003  0.745979 -0.388455 -0.683481  0.174722 -0.197217  1.423126  0.906270 -1.086627  0.907024  0.226867  0.266120  0.103830  0.304199 -0.127455  0.994761 -0.501053  0.039003  0.024135 -0.056241   0  \n",
       "     0.716662 -1.097812  0.629443  1.939510 -0.494739  1.819965 -0.624840  0.482657  0.783612  0.207105 -1.392283  0.330364 -0.574920 -0.737417 -2.643564  0.604758 -0.450969  0.219757  0.967780  0.388818 -0.263286 -0.978693 -0.359244 -1.332021  0.296656  0.937425 -0.068357  0.032839   0  \n",
       "491 -0.946412  0.609500  1.201710  0.113074 -0.210132 -0.954009  0.130325  0.293270 -0.579076 -0.756168 -0.627040  0.495057  0.646681  0.227179  0.319772 -0.368821  0.386365 -0.537638  1.125068  0.144104 -0.036421 -0.266529 -0.038869  0.481943 -0.277006  0.949985 -0.079616 -0.016606   0  \n",
       "     1.196288  0.217099  0.402596  0.921664 -0.252045 -0.392240 -0.047177  0.041339 -0.069318  0.169913  0.947708  0.417596 -0.798841  0.679509  0.453897  0.562062 -0.805390  0.367828  0.128334 -0.157752 -0.205751 -0.688869  0.054388 -0.056469  0.352684 -0.625998  0.013681  0.015683   0  \n",
       "     1.130024 -0.060944  0.662074  1.033990 -0.441560  0.310187 -0.559948  0.349136  0.332922  0.220886  0.826773 -0.257591 -1.961189  0.728867  1.004105  0.527214 -0.583410  0.631457 -0.573334 -0.258810  0.195852  0.511310 -0.100457 -0.344298  0.420079 -0.213097  0.040658  0.010599   0  \n",
       "492 -0.789890 -1.379371  0.171334 -1.636756 -2.807266  0.726236  2.737602 -0.933999 -2.413730  0.796625 -0.878071 -1.045044  1.249929 -0.960840 -0.507105 -0.831644  0.490803 -0.077389  0.298180 -0.129411 -0.256231  0.167077  0.540876  0.067496  0.479636 -0.098230  0.053527 -0.408050   0  \n",
       "493  1.219725 -0.481149 -0.324351 -1.552562 -0.258357 -0.519342 -0.035458 -0.117824  1.267125 -1.068542 -0.638446  0.352485  0.060428  0.332823  1.999236 -0.505571 -0.160606 -0.332378  0.646091  0.035718 -0.040428 -0.106938 -0.171758 -0.721597  0.525568  0.084212 -0.001621  0.011866   0  \n",
       "494  0.058501  1.058661  1.076857  1.589043 -0.400077 -0.677461  0.778466 -0.303557 -0.630844  0.230737  0.015912  0.549293  1.123662  0.146946  1.428015 -0.697686  0.108457 -0.015595  1.176153  0.104401  0.192879  0.702194  0.217738  0.736017 -1.276798 -0.461654  0.205261  0.250664   0  \n",
       "495 -0.239505 -3.940241 -0.147576 -0.671347 -2.239256  0.908178 -0.377398  0.157943 -1.595928  0.987881  1.225026 -0.139996 -0.453948 -0.111467 -0.490358 -0.973998  1.248488 -0.588332 -0.509182  1.217690  0.076296 -1.132178 -0.486820 -0.302911 -0.304121 -0.469811 -0.077517  0.151745   0  \n",
       "496 -0.606284 -0.090112  2.012027  0.352728 -0.210154  0.513719 -0.496402  0.379687 -1.663990  0.767278  0.743764 -0.250209 -0.405640  0.312433  1.090898 -1.509169 -0.009483  2.107139 -0.172341 -0.250196 -0.065436  0.148715 -0.060198 -0.356965 -0.213996 -0.143624  0.167360  0.122290   0  \n",
       "    -2.009893  0.522693  2.043510  0.223521 -0.597859  0.078163 -0.450347  0.772471  0.532385 -0.408490 -0.862066 -0.035498 -0.648890 -0.191864  0.423878 -0.106994  0.332679 -0.405176 -0.141551 -0.150668 -0.126642  0.080104  0.272598  0.088534  0.317082  0.419253  0.281494  0.060872   0  \n",
       "    -0.652955  0.493216  1.246770 -0.707569  0.165764 -1.146224  0.932049 -0.094962 -0.861258 -0.503549  1.448318  0.352719 -0.832586  0.721808 -0.002442  0.060623 -0.270581 -0.285535  0.757072  0.117608 -0.209224 -0.894706  0.167075  0.528922 -0.419801  0.629628 -0.040930  0.097624   0  \n",
       "497 -0.417836  0.981103  1.135820 -0.172593  0.396675 -0.002842  0.519780  0.248763 -0.593497 -0.382215  1.364965  0.429958 -0.220197 -0.165067  0.501964  0.396923 -0.066857  0.053123 -0.035596  0.098874 -0.205580 -0.547474 -0.020486 -0.368187 -0.227053  0.105281  0.253800  0.081276   0  \n",
       "498 -0.753769  1.098287  1.835453  0.614571 -0.317859 -0.101055  0.039454 -0.656824 -0.048761 -0.216191 -0.311959 -0.414791 -1.023195  0.270999  1.248325 -0.046737 -0.097763 -0.015157 -0.965601 -0.369512  1.101142  0.686059  0.005954  0.361744 -0.457852 -0.352576  0.094602  0.170480   0  \n",
       "499  1.255439  0.307729  0.292700  0.699873 -0.428876 -1.088456  0.043840 -0.167739  0.128854 -0.264062 -0.174508 -0.103537 -0.410713 -0.160753  1.163808  0.508362  0.031183 -0.221105 -0.194241 -0.121156 -0.294795 -0.882126  0.136846  0.327949  0.194459  0.096516 -0.027271  0.029491   0  \n",
       "     1.355790 -1.122921  1.191187 -0.576548 -1.708750  0.274721 -1.640785  0.295838  0.618649  0.475943 -1.772232 -1.558247 -1.276128 -0.750821  0.394213  1.563193  0.139541 -0.368044  0.765654  0.020548  0.302035  0.920013 -0.210219 -0.435499  0.450803  0.068454  0.062214  0.022191   0  \n",
       "    -0.860626 -0.109137  2.112474 -1.400567  0.180269  1.329656  0.431001  0.317761  0.593762 -0.987304 -0.309227  0.067846 -0.841815 -0.609413 -1.189738  0.329121 -0.627299  0.012316  0.257628  0.157201 -0.197635 -0.436535 -0.194840 -1.236873  0.218668  0.902383 -0.210195 -0.190458   0  \n",
       "\n",
       "[662 rows x 2 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>1.191857</th>\n",
       "      <th>0.266151</th>\n",
       "      <th>0.166480</th>\n",
       "      <th>0.448154</th>\n",
       "      <th>0.060018</th>\n",
       "      <th>-0.082361</th>\n",
       "      <th>-0.078803</th>\n",
       "      <th>0.085102</th>\n",
       "      <th>-0.255425</th>\n",
       "      <th>-0.166974</th>\n",
       "      <th>1.612727</th>\n",
       "      <th>1.065235</th>\n",
       "      <th>0.489095</th>\n",
       "      <th>-0.143772</th>\n",
       "      <th>0.635558</th>\n",
       "      <th>0.463917</th>\n",
       "      <th>-0.114805</th>\n",
       "      <th>-0.183361</th>\n",
       "      <th>-0.145783</th>\n",
       "      <th>-0.069083</th>\n",
       "      <th>-0.225775</th>\n",
       "      <th>-0.638672</th>\n",
       "      <th>0.101288</th>\n",
       "      <th>-0.339846</th>\n",
       "      <th>0.167170</th>\n",
       "      <th>0.125895</th>\n",
       "      <th>-0.008983</th>\n",
       "      <th>0.014724</th>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>-1.358354</th>\n",
       "      <th>-1.340163</th>\n",
       "      <th>1.773209</th>\n",
       "      <th>0.379780</th>\n",
       "      <th>-0.503198</th>\n",
       "      <th>1.800499</th>\n",
       "      <th>0.791461</th>\n",
       "      <th>0.247676</th>\n",
       "      <th>-1.514654</th>\n",
       "      <th>0.207643</th>\n",
       "      <th>0.624501</th>\n",
       "      <th>0.066084</th>\n",
       "      <th>0.717293</th>\n",
       "      <th>-0.165946</th>\n",
       "      <th>2.345865</th>\n",
       "      <th>-2.890083</th>\n",
       "      <th>1.109969</th>\n",
       "      <th>-0.121359</th>\n",
       "      <th>-2.261857</th>\n",
       "      <th>0.524980</th>\n",
       "      <th>0.247998</th>\n",
       "      <th>0.771679</th>\n",
       "      <th>0.909412</th>\n",
       "      <th>-0.689281</th>\n",
       "      <th>-0.327642</th>\n",
       "      <th>-0.139097</th>\n",
       "      <th>-0.055353</th>\n",
       "      <th>-0.059752</th>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-0.966272</th>\n",
       "      <th>-0.185226</th>\n",
       "      <th>1.792993</th>\n",
       "      <th>-0.863291</th>\n",
       "      <th>-0.010309</th>\n",
       "      <th>1.247203</th>\n",
       "      <th>0.237609</th>\n",
       "      <th>0.377436</th>\n",
       "      <th>-1.387024</th>\n",
       "      <th>-0.054952</th>\n",
       "      <th>-0.226487</th>\n",
       "      <th>0.178228</th>\n",
       "      <th>0.507757</th>\n",
       "      <th>-0.287924</th>\n",
       "      <th>-0.631418</th>\n",
       "      <th>-1.059647</th>\n",
       "      <th>-0.684093</th>\n",
       "      <th>1.965775</th>\n",
       "      <th>-1.232622</th>\n",
       "      <th>-0.208038</th>\n",
       "      <th>-0.108300</th>\n",
       "      <th>0.005274</th>\n",
       "      <th>-0.190321</th>\n",
       "      <th>-1.175575</th>\n",
       "      <th>0.647376</th>\n",
       "      <th>-0.221929</th>\n",
       "      <th>0.062723</th>\n",
       "      <th>0.061458</th>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>-1.158233</th>\n",
       "      <th>0.877737</th>\n",
       "      <th>1.548718</th>\n",
       "      <th>0.403034</th>\n",
       "      <th>-0.407193</th>\n",
       "      <th>0.095921</th>\n",
       "      <th>0.592941</th>\n",
       "      <th>-0.270533</th>\n",
       "      <th>0.817739</th>\n",
       "      <th>0.753074</th>\n",
       "      <th>-0.822843</th>\n",
       "      <th>0.538196</th>\n",
       "      <th>1.345852</th>\n",
       "      <th>-1.119670</th>\n",
       "      <th>0.175121</th>\n",
       "      <th>-0.451449</th>\n",
       "      <th>-0.237033</th>\n",
       "      <th>-0.038195</th>\n",
       "      <th>0.803487</th>\n",
       "      <th>0.408542</th>\n",
       "      <th>-0.009431</th>\n",
       "      <th>0.798278</th>\n",
       "      <th>-0.137458</th>\n",
       "      <th>0.141267</th>\n",
       "      <th>-0.206010</th>\n",
       "      <th>0.502292</th>\n",
       "      <th>0.219422</th>\n",
       "      <th>0.215153</th>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-0.425966</th>\n",
       "      <th>0.960523</th>\n",
       "      <th>1.141109</th>\n",
       "      <th>-0.168252</th>\n",
       "      <th>0.420987</th>\n",
       "      <th>-0.029728</th>\n",
       "      <th>0.476201</th>\n",
       "      <th>0.260314</th>\n",
       "      <th>-0.568671</th>\n",
       "      <th>-0.371407</th>\n",
       "      <th>1.341262</th>\n",
       "      <th>0.359894</th>\n",
       "      <th>-0.358091</th>\n",
       "      <th>-0.137134</th>\n",
       "      <th>0.517617</th>\n",
       "      <th>0.401726</th>\n",
       "      <th>-0.058133</th>\n",
       "      <th>0.068653</th>\n",
       "      <th>-0.033194</th>\n",
       "      <th>0.084968</th>\n",
       "      <th>-0.208254</th>\n",
       "      <th>-0.559825</th>\n",
       "      <th>-0.026398</th>\n",
       "      <th>-0.371427</th>\n",
       "      <th>-0.232794</th>\n",
       "      <th>0.105915</th>\n",
       "      <th>0.253844</th>\n",
       "      <th>0.081080</th>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                             0   \\\n",
       "0  1.191857  0.266151 0.166480  0.448154  0.060018 -0.082361 -0.078803  0.085102 -0.255425 -0.166974  1.612727 1.065235  0.489095 -0.143772  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "1 -1.358354 -1.340163 1.773209  0.379780 -0.503198  1.800499  0.791461  0.247676 -1.514654  0.207643  0.624501 0.066084  0.717293 -0.165946  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "  -0.966272 -0.185226 1.792993 -0.863291 -0.010309  1.247203  0.237609  0.377436 -1.387024 -0.054952 -0.226487 0.178228  0.507757 -0.287924 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "2 -1.158233  0.877737 1.548718  0.403034 -0.407193  0.095921  0.592941 -0.270533  0.817739  0.753074 -0.822843 0.538196  1.345852 -1.119670  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "  -0.425966  0.960523 1.141109 -0.168252  0.420987 -0.029728  0.476201  0.260314 -0.568671 -0.371407  1.341262 0.359894 -0.358091 -0.137134  0.517617  0.401726 -0.058133  0.068653 -0.033194  0.084968 -0.208254 -0.559825 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080    3.67   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                         31  \n",
       "0  1.191857  0.266151 0.166480  0.448154  0.060018 -0.082361 -0.078803  0.085102 -0.255425 -0.166974  1.612727 1.065235  0.489095 -0.143772  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   0  \n",
       "1 -1.358354 -1.340163 1.773209  0.379780 -0.503198  1.800499  0.791461  0.247676 -1.514654  0.207643  0.624501 0.066084  0.717293 -0.165946  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   0  \n",
       "  -0.966272 -0.185226 1.792993 -0.863291 -0.010309  1.247203  0.237609  0.377436 -1.387024 -0.054952 -0.226487 0.178228  0.507757 -0.287924 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   0  \n",
       "2 -1.158233  0.877737 1.548718  0.403034 -0.407193  0.095921  0.592941 -0.270533  0.817739  0.753074 -0.822843 0.538196  1.345852 -1.119670  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   0  \n",
       "  -0.425966  0.960523 1.141109 -0.168252  0.420987 -0.029728  0.476201  0.260314 -0.568671 -0.371407  1.341262 0.359894 -0.358091 -0.137134  0.517617  0.401726 -0.058133  0.068653 -0.033194  0.084968 -0.208254 -0.559825 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080   0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:,0:30].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x, y, test_size= .33, random_state= 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[211   0]\n",
      " [  0   8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       211\n",
      "           1       1.00      1.00      1.00         8\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       219\n",
      "   macro avg       1.00      1.00      1.00       219\n",
      "weighted avg       1.00      1.00      1.00       219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, pred))\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x153f7833470>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC curve evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 1.]), array([0., 1., 1.]), array([2, 1, 0], dtype=int64))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr,tpr,_= roc_curve(y_test,pred)\n",
    "fpr,tpr,_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc = roc_auc_score(y_test, pred)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr,tpr,label=\"data, auc=\"+str(auc))\n",
    "plt.legend(loc=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# Gradientsearch\n",
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\programdata\\anaconda3\\lib\\site-packages (0.90)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.16.2)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr= RandomForestClassifier()\n",
    "rfr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predrfr= rfr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predrfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ac = accuracy_score(y_test,predrfr)\n",
    "cf = confusion_matrix(y_test,predrfr)\n",
    "cr = classification_report(y_test,predrfr)\n",
    "ac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[211,   0],\n",
       "       [  0,   8]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00       211\\n           1       1.00      1.00      1.00         8\\n\\n   micro avg       1.00      1.00      1.00       219\\n   macro avg       1.00      1.00      1.00       219\\nweighted avg       1.00      1.00      1.00       219\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "xgb= xgboost.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit(x_train,y_train)\n",
    "predx= xgb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[211   0]\n",
      " [  0   8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       211\n",
      "           1       1.00      1.00      1.00         8\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       219\n",
      "   macro avg       1.00      1.00      1.00       219\n",
      "weighted avg       1.00      1.00      1.00       219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,predx))\n",
    "print(confusion_matrix(y_test,predx))\n",
    "print(classification_report(y_test,predx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.fit(x_train,y_train)\n",
    "gbc.score(x_train,y_train)\n",
    "predg= gbc.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[211   0]\n",
      " [  0   8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       211\n",
      "           1       1.00      1.00      1.00         8\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       219\n",
      "   macro avg       1.00      1.00      1.00       219\n",
      "weighted avg       1.00      1.00      1.00       219\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,predg))\n",
    "print(confusion_matrix(y_test,predg))\n",
    "print(classification_report(y_test,predg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = pickle.dumps(xgb) # saving data from jupyter to computer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_from_pickle= pickle.loads(saved_model) # loading data back into jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_from_pickle.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_credit.pkl']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(xgb, 'xgb_credit.pkl') # saving from jupyter to computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbload= joblib.load('xgb_credit.pkl') # from computer to jupyter\n",
    "xgbload.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
